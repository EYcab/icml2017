{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Interpretable Machine Learning](http://people.csail.mit.edu/beenkim/icml_tutorial.html)\n",
    "\n",
    "Sun, 8/5/2017\n",
    "\n",
    "Been Kim (Google Brain)\n",
    "\n",
    "Finale Doshi-Velez (Harvard)\n",
    "\n",
    "*As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination.*\n",
    "\n",
    "Relevant papers:\n",
    "\n",
    "1. [*Interactive and Interpretable Machine Learning Models for Human Machine Collaboration*](http://people.csail.mit.edu/beenkim/papers/BKimPhDThesis.pdf)\n",
    "2. [*Examples are Not Enough, Learn to Criticize! Criticism for Interpretability*](http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf)\n",
    "3. [*Towards A Rigorous Science of Interpretable Machine Learning*](https://arxiv.org/pdf/1702.08608.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning is a powerful tool with serious consequences\n",
    "\n",
    "* Cost-effective Health Care built models to predict probability of death for patients (Cooper et al. 97)\n",
    "  * Trained two models:\n",
    "    * Neural network\n",
    "    * Logistic regression\n",
    "  * Neural net performed better, however, logistic regression extracted this pattern:\n",
    "    * `HasAsthma(x) -> LowerRisk for pneumonia(x)`\n",
    "  * So, knowing neural nets performed better, what *other* rules did it learn? __And how can we find out?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this relevant now?\n",
    "\n",
    "* Widespread data collection + vast computation resources\n",
    "  * ML everywhere!\n",
    "  \n",
    "### Example\n",
    "\n",
    "*Why not just use a decision tree?*\n",
    "* Can you explain the logic of the tree build (i.e., the *actual* rationale)?\n",
    "* Can you guess which feature was \"more important?\"\n",
    "\n",
    "*Well, what if we learn rule sets instead?*\n",
    "* Can still be extremely complex:\n",
    "```\n",
    "IF (sunny and hot) OR (cold and snowing) AND NOT (...)\n",
    "```\n",
    "\n",
    "### So do decision trees/rule lists, etc. not work?\n",
    "\n",
    "* __They may work for your use case!__\n",
    "* *However*, there is no one right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is interpretability\n",
    "\n",
    "* Interpretation is the process of giving __explanations__ (TO HUMANS)\n",
    "\n",
    "  ### 1. Why and when?\n",
    "    - When there is fundamental __underspecification__ in the problem\n",
    "      - More data or more clever algorithm will not help\n",
    "      - E.g., safety in autonomous vehicles (what is safety?)\n",
    "        - Not hitting a person?\n",
    "        - Stopping within 2 feet of someone?\n",
    "      - E.g., debugging\n",
    "        - Why doesn't something work?\n",
    "      - E.g., mismatched objectives and multi-objective trade-offs\n",
    "        - What you optimize is not what you meant to optimize\n",
    "      - E.g., science\n",
    "        - You want to discover something new, but you don't know what it is.\n",
    "      - E.g., __legal/ethics__\n",
    "        - We're legally required for regulatory purposed to provide an explanation\n",
    "        \n",
    "    - When would we __not__ want interpretability (or, rather, when is it not as important)?\n",
    "      - No significant consequences\n",
    "      - Sufficiently well-studied problem\n",
    "      - Prevent gaming the system\n",
    "      \n",
    "  ### 2. How can we achieve interpretability?\n",
    "    - Three stages:\n",
    "    \n",
    "      #### a. Prior to building models\n",
    "        - Visualization (*note to reader: check out Google's new open source [\"Facets\"](https://pair-code.github.io/facets/) tool!*)\n",
    "        - Exploratory data analysis\n",
    "        \n",
    "      #### b. When building a new model\n",
    "        - Select an interpretable method:\n",
    "        \n",
    "          i. Rule-based, per-feature-based (parametric)\n",
    "            - __However__, subject to data density and mis-represented subpopulations!\n",
    "            - It might not be as interpretable as you think (imagine a decision tree of depth 10&mdash;how do you really rationalize what's going on?)\n",
    "            - Each feature must be independently interpretable for model to be interpretable\n",
    "            \n",
    "          ii. Case-based (similar to clustering)\n",
    "            - \"I recommend treatment X because it worked for patients like you\"\n",
    "            - __However__, there may not be good, representative examples\n",
    "            - Humans might over-generalize\n",
    "            \n",
    "          iii. Sparsity-based\n",
    "            - Model correlations across subtrees\n",
    "            - __However__, just because it's sparse doesn't mean it's interpretable. Over-sparsity might spawn randomness.\n",
    "            \n",
    "          iv. Monotonicity\n",
    "            - Learn piecewise monotonic functions\n",
    "          \n",
    "      #### c. After building a model (if you already have one):\n",
    "      \n",
    "        - Sensitivity analysis, gradient-based methods\n",
    "          - \"What would happen to output $y$ if we perturb the input $x \\rightarrow x + \\epsilon$\" (see Riberiro et al. '16)\n",
    "          - See Koh et al. '17\n",
    "          \n",
    "        - Saliency/attribution maps:\n",
    "          - Give me the features in the input space that mattered for the classification ($\\frac{\\partial y}{\\partial x_{ij}}$)\n",
    "          \n",
    "          - Derivative might not sum up for non-linear functions\n",
    "          - __However__, model may not allow sensitivity analysis.\n",
    "          \n",
    "        - Mimic models\n",
    "          - Model compression or distillation (Bucila et al. '06, Ba et al. '14, Hinton et al. '15)\n",
    "          - Visual explanations (Hendricks et al. '16)\n",
    "          - __However__, you might not be able to distill (there may not be a simpler model at all), and there might be a gap between what the actual model is doing and what your mimic model is doing\n",
    "        \n",
    "        - Investigation on hidden layers\n",
    "          - Deep dream\n",
    "          - Deconvolution net\n",
    "          - Network dissection\n",
    "          - __However__, there may be a lack of actional insights\n",
    "          \n",
    "  ### 3. How do we measure the explanation quality (i.e., *what is good?*)\n",
    "    - \"You know it when you see it\"\n",
    "    - Benchmark against human performance\n",
    "    - We need something more general...\n",
    "    \n",
    "      - Function-based\n",
    "        - Can we use some proxy such as sparsity, monotonicity, or non-negativity?\n",
    "        - Easy to formalize, optimize and evaluate (*but might not solve a real world need!*)\n",
    "        \n",
    "      - Application based\n",
    "        - Does providing interpretability assist with down-stream tassks, such as increasing fairness, safety, scientific discovery, or productivity?\n",
    "        - Can be very costly and difficult to compare work A to B!\n",
    "        \n",
    "      - __Cognition-based__\n",
    "        - What factor should change to change the outcome?\n",
    "        - What are the discriminative features?\n",
    "        - `What [INPUT|WEIGHT|COST] would change the [PREDICTIONS FOR|CLUSTER OF] x?`\n",
    "        - E.g., forward simulation\n",
    "\n",
    "### Cognition-based interpretability efficacy\n",
    "\n",
    "1. Problem-related factors\n",
    "  - Global vs. local\n",
    "    - Local meaning related to the specific observation (\"what happened to this guy? Why this prediction?\")\n",
    "  \n",
    "  - Time budget\n",
    "    - Different problems have a different time criticality\n",
    "  \n",
    "  - Severity of underspecifcation\n",
    "    - How much risk does ambiguity pose?\n",
    "    \n",
    "2. Method-related factors\n",
    "  - Cognitive chunks\n",
    "    - Representation of information that you get to choose\n",
    "    \n",
    "  - Audience training\n",
    "    - The expert's background will affect what cognitive chunks are selected/identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways for State Farm\n",
    "\n",
    "Obviously State Farm has a huge need for model interpretability in order to overcome model regulatory challenges and to get through ERM model validation. However, Been raises a good point: *how do we define model interpretability?* \n",
    "\n",
    "Traditionally, and mostly to the actuarial team, this means just using GLM or other simple, linear models that have intrinsic feature selection terms/penalties and feature importances. But, perhaps altering how we look at model interpretability could allow our data scientists to build more complex models so long as we explore interpretability practices such as sensitivity analysis (or partial dependency maps, etc.). This also colors the way we design features for our models; if each feature is not independently \"interpretable\" or if the subspace density is too small, our interpretation of the model might be impacted (i.e., pneumonia risk in adults > 100&mdash;risk doesn't actually decrease, we just see less observations of adults within this demographic due to other factors).\n",
    "\n",
    "__Final considerations__\n",
    "1. Models should not be interpretable for the sake of being so, since it's an expensive, convoluted process to validate. Only models that truly require interpretability should make these considerations\n",
    "2. Features should be examined (within reason) for regions of subspace sparsity, and that should be considered when trying to explain a model\n",
    "3. Specifically consider \"audience training\" when interpreting models for business partners, and look for their validation while being aware of their potential biases\n",
    "4. Know what granularity really should be reasonable for model interpretability&mdash;do you need to actually look at each individual observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
