{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "- Complex environments are required for complex learning\n",
    "  - Many human games repurposed as learning environments for these reasons:\n",
    "    - If it's a human game, we know it's interesting, challenging and relevant\n",
    "    - Incredible diversity for environments. Can test different parts of human intelligence\n",
    "    \n",
    "### Subgoals\n",
    "If a problem can be broken into subgoals, we can liearn sub-policies\n",
    "  - But we don't want to have to define subgoals\n",
    "  \n",
    "*Hierarchical reinforcement learning*\n",
    "- Structured exploration\n",
    "- Transfer learning\n",
    "\n",
    "### Feudal RL\n",
    "P Dayan, G Hinton '03\n",
    "- Create hierarchy of policies, so that we have increasingly higher level of abstraction in policies, and lower level of temporal resolution\n",
    "\n",
    "- FeUdal Nets (FUN)\n",
    "  - Manager and a worker, both fed by output from CNN\n",
    "  - Manager contains dilated LSTM\n",
    "    - Output is goals (state representation)\n",
    "  - Goals passed to worker LSTM\n",
    "    - Tries to match goals\n",
    "    - Creates actions in the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual learning\n",
    "Environment complexity through many tasks\n",
    "\n",
    "### Catastrophic forgetting\n",
    "- Well known phenomenon\n",
    "- Especially severe in deep RL\n",
    "  - Pong example (changing colors leads to catastrophic learning)\n",
    "- L2 regularization does not help--just slows down learning\n",
    "- Elastic weight consolidation\n",
    "  - Add regularizer that adds a penalty and can retain weights pertaining to task A, while network adapts to learn how to handle task B\n",
    "  \n",
    "### What about tasks that don't get along?\n",
    "1. Progressive nets\n",
    "  - Add columns for new tasks\n",
    "  - Freeze params of learned columns\n",
    "  - layer-wise neural connections\n",
    "  - Allows transfer learning\n",
    "  \n",
    "2. Distral (Distill and Transfer learning)\n",
    "  - Task-specific networks plus shared network\n",
    "  - KL divergence constraint\n",
    "  - Regularization in policy space rather than parameter space\n",
    "  - Shared policy as a communication channel between tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary tasks to speed up RL\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
