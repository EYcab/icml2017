{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human in the Loop\n",
    "\n",
    "*As machine learning systems become more ubiquitous in everybody’s day-to-day life or work, society and industry is in an intermediate state between fully manual and fully automatic systems. The gradient undoubtedly points towards full automation, but moving forward in this direction is going to face increasing challenges due to the fact that current machine learning research tends to focus on end to end systems, which puts aside the fact that for practical applications there are still gaps or caveats in the automation. Parts of these come from the presence of (or the necessity to have) the Human in the Loop.*\n",
    "\n",
    "*There are two main locations for the Human in the automated system: (i) upstream, in which case the focus is mainly in the inputs of the algorithm. This can be essential for personalised assistants, that describe environments where the machine learning method is tightly embedded into the system. Such environments pose additional challenges related to privacy at large; (ii) downstream: other domains have machine learning approaches analyse parts of the data, and human experts use the results and intuition to make decisions.*\n",
    "\n",
    "*The Human dependences between these two locations is also neither straightforward nor acyclic — some applications tend to have feedback effects on data as actions or interventions are undertaken based on machine learning predictions. Furthermore there are often very few rounds of decision making in practice, but each round may affect the statement of the problems related to the Human presence, as witnessed for example by eventual privacy leakages.*\n",
    "\n",
    "*This workshop aims to bring together people who are working on systems where machine learning is only part of the solution. Participants will exchange ideas and experiences on human in the loop machine learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive and Interpretable Machine Learning Models\n",
    "\n",
    "Been Kim, Google Brain\n",
    "\n",
    "Paper: [Examples Are Not Enough! Learn to Criticize](http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf)\n",
    "\n",
    "\n",
    "### Vision\n",
    "*Harness the relative strength of humans and machine learning models*\n",
    "- How?\n",
    "  - Develop methods inspired by how humans think and that make sense to humans.\n",
    "  - Interact with humans.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Communication from data to human\n",
    "\n",
    "  - Exploratory data analysis\n",
    "    - Make sense of the data itself prior to doing any analytics\n",
    "    - What does the data look like?\n",
    "      - Strategic sampling\n",
    "      \n",
    "    - __MMD-critic__\n",
    "      - Algorithm to select most archetypcal samples for human to understand the data\n",
    "      - Cohen 96, Newell 72\n",
    "      - Klein 89 (fire fighters)\n",
    "      - Mirror the way humans think\n",
    "        - __Problem__: humans over-generalize\n",
    "          - over-generalization is consistent with evolutionary theory (Zebrowitz '10, Schaller '06)\n",
    "          - Algorithms can help against over-generalization\n",
    "          \n",
    "      - Select $p$, the prototypes, such that $p$ is most similar to the distribution of the data. Then, select $q$, the criticisms, that maximize the difference in some points and $p$. Use MMD (maximum mean discrepancy): $$TODO$$\n",
    "      \n",
    "      - Empirically can be measured using samples\n",
    "      - Algorithm:\n",
    "        1. Choose number of prototypes and criticisms\n",
    "        2. Select prototypes using greedy search\n",
    "        3. Select criticisms using greedy search\n",
    "<br/><br/>\n",
    "2. Communication from machine to human\n",
    "    - Case-based reasoning\n",
    "      - Learn from past instances, with slightly modified new experience\n",
    "      - However, requires strictly supervised labels\n",
    "      - Does not scale to complex problems (lack of intuitive power)\n",
    "      - Does not leverage patterns of data\n",
    "      \n",
    "    - Interpretable models\n",
    "      - Decision tree\n",
    "      - Sparse linear classifiers\n",
    "      \n",
    "    - Bayesian case model (BCM)\n",
    "      - Leverage power of examples (prototypes), combine case-based reasoning and interpretable models.\n",
    "      - Clustering method that performs sparse clustering of patterns in sub-spaces\n",
    "      - Since real-world data does not contain labels, BCM finds ways to explain correlations in sparse feature subspaces\n",
    "      - Very similar to Latent Dirichlet Allocation\n",
    "      - If a sample shares a sparse sub-space feature with a protype example, score higher for that class\n",
    "      - Are we sacrificing anything for interpretability?\n",
    "        - If you craft the model such that it fits your model well, it's possible to find parsimony between performance and interpretability\n",
    "<br/><br/>\n",
    "3. Communication from human to machine: incorporate feedback\n",
    "    - Defining practical implications of model parameters (i.e., *k* in clustering)\n",
    "    - Humans generally do not know what would be the best for them\n",
    "    - iBCM for introductory programming education\n",
    "      - Why education?\n",
    "        - Educators generally staunch in how they define curriculums, hw, etc.\n",
    "      - Extract the \"right features\" for defining prototypical code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
